/**
    * @license
    * Copyright 2020 Google LLC. All Rights Reserved.
    * Licensed under the Apache License, Version 2.0 (the "License");
    * you may not use this file except in compliance with the License.
    * You may obtain a copy of the License at
    *
    * http://www.apache.org/licenses/LICENSE-2.0
    *
    * Unless required by applicable law or agreed to in writing, software
    * distributed under the License is distributed on an "AS IS" BASIS,
    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    * See the License for the specific language governing permissions and
    * limitations under the License.
    * =============================================================================
    */
import{tensor2d,tensor1d,util,tidy}from"@tensorflow/tfjs-core";import{loadGraphModel}from"@tensorflow/tfjs-converter";const stringToChars=e=>{const t=[];for(const n of e)t.push(n);return t};class TrieNode{constructor(){this.parent=null,this.children={},this.end=!1,this.word=[[],0,0]}}class Trie{constructor(){this.root=new TrieNode}insert(e,t,n){let o=this.root;const s=stringToChars(e);for(let e=0;e<s.length;e++)o.children[s[e]]||(o.children[s[e]]=new TrieNode,o.children[s[e]].parent=o,o.children[s[e]].word[0]=o.word[0].concat(s[e])),o=o.children[s[e]],e===s.length-1&&(o.end=!0,o.word[1]=t,o.word[2]=n)}commonPrefixSearch(e){const t=[];let n=this.root.children[e[0]];for(let o=0;o<e.length&&n;o++)n.end&&t.push(n.word),n=n.children[e[o+1]];return t.length||t.push([[e[0]],0,0]),t}}const separator="▁";function processInput(e){const t=e.normalize("NFKC");return t.length>0?separator+t.replace(/ /g,separator):t}const RESERVED_SYMBOLS_COUNT=6;class Tokenizer{constructor(e,t=RESERVED_SYMBOLS_COUNT){this.vocabulary=e,this.reservedSymbolsCount=t,this.trie=new Trie;for(let e=this.reservedSymbolsCount;e<this.vocabulary.length;e++)this.trie.insert(this.vocabulary[e][0],this.vocabulary[e][1],e)}encode(e){const t=[],n=[],o=[];e=processInput(e);const s=stringToChars(e);for(let e=0;e<=s.length;e++)t.push({}),n.push(0),o.push(0);for(let e=0;e<s.length;e++){const n=this.trie.commonPrefixSearch(s.slice(e));for(let o=0;o<n.length;o++){const s=n[o],r={key:s[0],score:s[1],index:s[2]},l=s[0].length;null==t[e+l][e]&&(t[e+l][e]=[]),t[e+l][e].push(r)}}for(let e=0;e<=s.length;e++)for(const s in t[e]){const r=t[e][s];for(let t=0;t<r.length;t++){const s=r[t],l=s.score+o[e-s.key.length];(0===o[e]||l>=o[e])&&(o[e]=l,n[e]=r[t].index)}}const r=[];let l=n.length-1;for(;l>0;)r.push(n[l]),l-=this.vocabulary[n[l]][0].length;const i=[];let c=!1;for(let e=0;e<r.length;e++){const t=r[e];c&&0===t||i.push(t),c=0===t}return i.reverse()}}async function loadVocabulary(e){return(await util.fetch(e)).json()}const version="1.3.2",BASE_PATH="https://tfhub.dev/google/tfjs-model/universal-sentence-encoder-qa-ondevice/1",SKIP_VALUES=[0,1,2],OFFSET=3,INPUT_LIMIT=192,QUERY_NODE_NAME="input_inp_text",RESPONSE_CONTEXT_NODE_NAME="input_res_context",RESPONSE_NODE_NAME="input_res_text",RESPONSE_RESULT_NODE_NAME="Final/EncodeResult/mul",QUERY_RESULT_NODE_NAME="Final/EncodeQuery/mul",RESERVED_SYMBOLS_COUNT$1=3,TOKEN_PADDING=2,TOKEN_START_VALUE=1;async function loadQnA(){const e=new UniversalSentenceEncoderQnA;return await e.load(),e}class UniversalSentenceEncoderQnA{async loadModel(){return loadGraphModel(BASE_PATH,{fromTFHub:!0})}async load(){const[e,t]=await Promise.all([this.loadModel(),loadVocabulary(`${BASE_PATH}/vocab.json?tfjs-format=file`)]);this.model=e,this.tokenizer=new Tokenizer(t,RESERVED_SYMBOLS_COUNT$1)}embed(e){const t=tidy(()=>{const t=this.tokenizeStrings(e.queries,INPUT_LIMIT),n=this.tokenizeStrings(e.responses,INPUT_LIMIT);if(null!=e.contexts&&e.contexts.length!==e.responses.length)throw new Error("The length of response strings and context strings need to match.");const o=e.contexts||[];null==e.contexts&&(o.length=e.responses.length,o.fill(""));const s=this.tokenizeStrings(o,INPUT_LIMIT),r={};return r[QUERY_NODE_NAME]=t,r[RESPONSE_NODE_NAME]=n,r[RESPONSE_CONTEXT_NODE_NAME]=s,this.model.execute(r,[QUERY_RESULT_NODE_NAME,RESPONSE_RESULT_NODE_NAME])});return{queryEmbedding:t[0],responseEmbedding:t[1]}}tokenizeStrings(e,t){const n=e.map(e=>this.shiftTokens(this.tokenizer.encode(e),INPUT_LIMIT));return tensor2d(n,[e.length,INPUT_LIMIT],"int32")}shiftTokens(e,t){e.unshift(TOKEN_START_VALUE);for(let n=0;n<t;n++)n>=e.length?e[n]=TOKEN_PADDING:SKIP_VALUES.includes(e[n])||(e[n]+=OFFSET);return e.slice(0,t)}}const BASE_PATH$1="https://storage.googleapis.com/tfjs-models/savedmodel/universal_sentence_encoder";async function load(){const e=new UniversalSentenceEncoder;return await e.load(),e}class UniversalSentenceEncoder{async loadModel(){return loadGraphModel("https://tfhub.dev/tensorflow/tfjs-model/universal-sentence-encoder-lite/1/default/1",{fromTFHub:!0})}async load(){const[e,t]=await Promise.all([this.loadModel(),loadVocabulary(`${BASE_PATH$1}/vocab.json`)]);this.model=e,this.tokenizer=new Tokenizer(t)}async embed(e){"string"==typeof e&&(e=[e]);const t=e.map(e=>this.tokenizer.encode(e)),n=t.map((e,t)=>e.map((e,n)=>[t,n]));let o=[];for(let e=0;e<n.length;e++)o=o.concat(n[e]);const s=tensor2d(o,[o.length,2],"int32"),r=tensor1d(util.flatten(t),"int32"),l={indices:s,values:r},i=await this.model.executeAsync(l);return s.dispose(),r.dispose(),i}}export{load,UniversalSentenceEncoder,Tokenizer,loadQnA,version};

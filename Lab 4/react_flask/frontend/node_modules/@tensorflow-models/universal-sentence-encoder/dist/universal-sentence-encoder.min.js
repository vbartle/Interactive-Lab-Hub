/**
    * @license
    * Copyright 2020 Google LLC. All Rights Reserved.
    * Licensed under the Apache License, Version 2.0 (the "License");
    * you may not use this file except in compliance with the License.
    * You may obtain a copy of the License at
    *
    * http://www.apache.org/licenses/LICENSE-2.0
    *
    * Unless required by applicable law or agreed to in writing, software
    * distributed under the License is distributed on an "AS IS" BASIS,
    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    * See the License for the specific language governing permissions and
    * limitations under the License.
    * =============================================================================
    */
!function(e,t){"object"==typeof exports&&"undefined"!=typeof module?t(exports,require("@tensorflow/tfjs-core"),require("@tensorflow/tfjs-converter")):"function"==typeof define&&define.amd?define(["exports","@tensorflow/tfjs-core","@tensorflow/tfjs-converter"],t):t(e.use={},e.tf,e.tf)}(this,function(e,t,n){"use strict";const o=e=>{const t=[];for(const n of e)t.push(n);return t};class s{constructor(){this.parent=null,this.children={},this.end=!1,this.word=[[],0,0]}}class r{constructor(){this.root=new s}insert(e,t,n){let r=this.root;const l=o(e);for(let e=0;e<l.length;e++)r.children[l[e]]||(r.children[l[e]]=new s,r.children[l[e]].parent=r,r.children[l[e]].word[0]=r.word[0].concat(l[e])),r=r.children[l[e]],e===l.length-1&&(r.end=!0,r.word[1]=t,r.word[2]=n)}commonPrefixSearch(e){const t=[];let n=this.root.children[e[0]];for(let o=0;o<e.length&&n;o++)n.end&&t.push(n.word),n=n.children[e[o+1]];return t.length||t.push([[e[0]],0,0]),t}}const l="▁";const i=6;class c{constructor(e,t=i){this.vocabulary=e,this.reservedSymbolsCount=t,this.trie=new r;for(let e=this.reservedSymbolsCount;e<this.vocabulary.length;e++)this.trie.insert(this.vocabulary[e][0],this.vocabulary[e][1],e)}encode(e){const t=[],n=[],s=[];e=function(e){const t=e.normalize("NFKC");return t.length>0?l+t.replace(/ /g,l):t}(e);const r=o(e);for(let e=0;e<=r.length;e++)t.push({}),n.push(0),s.push(0);for(let e=0;e<r.length;e++){const n=this.trie.commonPrefixSearch(r.slice(e));for(let o=0;o<n.length;o++){const s=n[o],r={key:s[0],score:s[1],index:s[2]},l=s[0].length;null==t[e+l][e]&&(t[e+l][e]=[]),t[e+l][e].push(r)}}for(let e=0;e<=r.length;e++)for(const o in t[e]){const r=t[e][o];for(let t=0;t<r.length;t++){const o=r[t],l=o.score+s[e-o.key.length];(0===s[e]||l>=s[e])&&(s[e]=l,n[e]=r[t].index)}}const i=[];let c=n.length-1;for(;c>0;)i.push(n[c]),c-=this.vocabulary[n[c]][0].length;const h=[];let d=!1;for(let e=0;e<i.length;e++){const t=i[e];d&&0===t||h.push(t),d=0===t}return h.reverse()}}async function h(e){return(await t.util.fetch(e)).json()}const d="https://tfhub.dev/google/tfjs-model/universal-sentence-encoder-qa-ondevice/1",a=[0,1,2],u=3,f=192,p="input_inp_text",g="input_res_context",m="input_res_text",w="Final/EncodeResult/mul",y="Final/EncodeQuery/mul",v=3,b=2,x=1;class j{async loadModel(){return n.loadGraphModel(d,{fromTFHub:!0})}async load(){const[e,t]=await Promise.all([this.loadModel(),h(`${d}/vocab.json?tfjs-format=file`)]);this.model=e,this.tokenizer=new c(t,v)}embed(e){const n=t.tidy(()=>{const t=this.tokenizeStrings(e.queries,f),n=this.tokenizeStrings(e.responses,f);if(null!=e.contexts&&e.contexts.length!==e.responses.length)throw new Error("The length of response strings and context strings need to match.");const o=e.contexts||[];null==e.contexts&&(o.length=e.responses.length,o.fill(""));const s=this.tokenizeStrings(o,f),r={};return r[p]=t,r[m]=n,r[g]=s,this.model.execute(r,[y,w])});return{queryEmbedding:n[0],responseEmbedding:n[1]}}tokenizeStrings(e,n){const o=e.map(e=>this.shiftTokens(this.tokenizer.encode(e),f));return t.tensor2d(o,[e.length,f],"int32")}shiftTokens(e,t){e.unshift(x);for(let n=0;n<t;n++)n>=e.length?e[n]=b:a.includes(e[n])||(e[n]+=u);return e.slice(0,t)}}const k="https://storage.googleapis.com/tfjs-models/savedmodel/universal_sentence_encoder";class z{async loadModel(){return n.loadGraphModel("https://tfhub.dev/tensorflow/tfjs-model/universal-sentence-encoder-lite/1/default/1",{fromTFHub:!0})}async load(){const[e,t]=await Promise.all([this.loadModel(),h(`${k}/vocab.json`)]);this.model=e,this.tokenizer=new c(t)}async embed(e){"string"==typeof e&&(e=[e]);const n=e.map(e=>this.tokenizer.encode(e)),o=n.map((e,t)=>e.map((e,n)=>[t,n]));let s=[];for(let e=0;e<o.length;e++)s=s.concat(o[e]);const r=t.tensor2d(s,[s.length,2],"int32"),l=t.tensor1d(t.util.flatten(n),"int32"),i={indices:r,values:l},c=await this.model.executeAsync(i);return r.dispose(),l.dispose(),c}}e.load=async function(){const e=new z;return await e.load(),e},e.UniversalSentenceEncoder=z,e.Tokenizer=c,e.loadQnA=async function(){const e=new j;return await e.load(),e},e.version="1.3.2",Object.defineProperty(e,"__esModule",{value:!0})});

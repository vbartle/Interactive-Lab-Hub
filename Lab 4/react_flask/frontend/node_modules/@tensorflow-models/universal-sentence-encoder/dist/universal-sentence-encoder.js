/**
    * @license
    * Copyright 2020 Google LLC. All Rights Reserved.
    * Licensed under the Apache License, Version 2.0 (the "License");
    * you may not use this file except in compliance with the License.
    * You may obtain a copy of the License at
    *
    * http://www.apache.org/licenses/LICENSE-2.0
    *
    * Unless required by applicable law or agreed to in writing, software
    * distributed under the License is distributed on an "AS IS" BASIS,
    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    * See the License for the specific language governing permissions and
    * limitations under the License.
    * =============================================================================
    */
(function (global, factory) {
  typeof exports === 'object' && typeof module !== 'undefined' ? factory(exports, require('@tensorflow/tfjs-core'), require('@tensorflow/tfjs-converter')) :
  typeof define === 'function' && define.amd ? define(['exports', '@tensorflow/tfjs-core', '@tensorflow/tfjs-converter'], factory) :
  (factory((global.use = {}),global.tf,global.tf));
}(this, (function (exports,tf,tfconv) { 'use strict';

  /**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   */
  // unicode-aware iteration
  const stringToChars = (input) => {
      const symbols = [];
      for (const symbol of input) {
          symbols.push(symbol);
      }
      return symbols;
  };

  /**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   */
  class TrieNode {
      constructor() {
          this.parent = null;
          this.children = {};
          this.end = false;
          this.word = [[], 0, 0];
      }
  }
  class Trie {
      constructor() {
          this.root = new TrieNode();
      }
      /**
       * Inserts a token into the trie.
       */
      insert(word, score, index) {
          let node = this.root;
          const symbols = stringToChars(word);
          for (let i = 0; i < symbols.length; i++) {
              if (!node.children[symbols[i]]) {
                  node.children[symbols[i]] = new TrieNode();
                  node.children[symbols[i]].parent = node;
                  node.children[symbols[i]].word[0] = node.word[0].concat(symbols[i]);
              }
              node = node.children[symbols[i]];
              if (i === symbols.length - 1) {
                  node.end = true;
                  node.word[1] = score;
                  node.word[2] = index;
              }
          }
      }
      /**
       * Returns an array of all tokens starting with ss.
       *
       * @param ss The prefix to match on.
       */
      commonPrefixSearch(ss) {
          const output = [];
          let node = this.root.children[ss[0]];
          for (let i = 0; i < ss.length && node; i++) {
              if (node.end) {
                  output.push(node.word);
              }
              node = node.children[ss[i + 1]];
          }
          if (!output.length) {
              output.push([[ss[0]], 0, 0]);
          }
          return output;
      }
  }

  /**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   */
  const separator = '\u2581'; // This is the unicode character 'lower one eighth block'.
  function processInput(str) {
      const normalized = str.normalize('NFKC');
      return normalized.length > 0 ?
          separator + normalized.replace(/ /g, separator) :
          normalized;
  }
  // The first tokens are reserved for unk, control symbols, and user-defined
  // symbols.
  const RESERVED_SYMBOLS_COUNT = 6;
  class Tokenizer {
      constructor(vocabulary, reservedSymbolsCount = RESERVED_SYMBOLS_COUNT) {
          this.vocabulary = vocabulary;
          this.reservedSymbolsCount = reservedSymbolsCount;
          this.trie = new Trie();
          for (let i = this.reservedSymbolsCount; i < this.vocabulary.length; i++) {
              this.trie.insert(this.vocabulary[i][0], this.vocabulary[i][1], i);
          }
      }
      encode(input) {
          const nodes = [];
          const words = [];
          const best = [];
          input = processInput(input);
          const symbols = stringToChars(input);
          for (let i = 0; i <= symbols.length; i++) {
              nodes.push({});
              words.push(0);
              best.push(0);
          }
          // Construct the lattice.
          for (let i = 0; i < symbols.length; i++) {
              const matches = this.trie.commonPrefixSearch(symbols.slice(i));
              for (let j = 0; j < matches.length; j++) {
                  const piece = matches[j];
                  const obj = { key: piece[0], score: piece[1], index: piece[2] };
                  const endPos = piece[0].length;
                  if (nodes[i + endPos][i] == null) {
                      nodes[i + endPos][i] = [];
                  }
                  nodes[i + endPos][i].push(obj);
              }
          }
          for (let endPos = 0; endPos <= symbols.length; endPos++) {
              for (const startPos in nodes[endPos]) {
                  const arr = nodes[endPos][startPos];
                  for (let j = 0; j < arr.length; j++) {
                      const word = arr[j];
                      const score = word.score + best[endPos - word.key.length];
                      if (best[endPos] === 0 || score >= best[endPos]) {
                          best[endPos] = score;
                          words[endPos] = arr[j].index;
                      }
                  }
              }
          }
          const results = [];
          // Backward pass.
          let iter = words.length - 1;
          while (iter > 0) {
              results.push(words[iter]);
              iter -= this.vocabulary[words[iter]][0].length;
          }
          // Merge consecutive unks.
          const merged = [];
          let isPreviousUnk = false;
          for (let i = 0; i < results.length; i++) {
              const id = results[i];
              if (!(isPreviousUnk && id === 0)) {
                  merged.push(id);
              }
              isPreviousUnk = id === 0;
          }
          return merged.reverse();
      }
  }
  /**
   * Load a vocabulary for the Tokenizer.
   *
   * @param pathToVocabulary Defaults to the path to the 8k vocabulary used by the
   * UniversalSentenceEncoder.
   */
  async function loadVocabulary(pathToVocabulary) {
      const vocabulary = await tf.util.fetch(pathToVocabulary);
      return vocabulary.json();
  }

  /** @license See the LICENSE file. */
  // This code is auto-generated, do not modify this file!
  const version = '1.3.2';

  /**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the 'License');
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an 'AS IS' BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   */
  const BASE_PATH = 'https://tfhub.dev/google/tfjs-model/universal-sentence-encoder-qa-ondevice/1';
  // Index in the vocab file that needs to be skipped.
  const SKIP_VALUES = [0, 1, 2];
  // Offset value for skipped vocab index.
  const OFFSET = 3;
  // Input tensor size limit.
  const INPUT_LIMIT = 192;
  // Model node name for query.
  const QUERY_NODE_NAME = 'input_inp_text';
  // Model node name for query.
  const RESPONSE_CONTEXT_NODE_NAME = 'input_res_context';
  // Model node name for response.
  const RESPONSE_NODE_NAME = 'input_res_text';
  // Model node name for response result.
  const RESPONSE_RESULT_NODE_NAME = 'Final/EncodeResult/mul';
  // Model node name for query result.
  const QUERY_RESULT_NODE_NAME = 'Final/EncodeQuery/mul';
  // Reserved symbol count for tokenizer.
  const RESERVED_SYMBOLS_COUNT$1 = 3;
  // Value for token padding
  const TOKEN_PADDING = 2;
  // Start value for each token
  const TOKEN_START_VALUE = 1;
  async function loadQnA() {
      const use = new UniversalSentenceEncoderQnA();
      await use.load();
      return use;
  }
  class UniversalSentenceEncoderQnA {
      async loadModel() {
          return tfconv.loadGraphModel(BASE_PATH, { fromTFHub: true });
      }
      async load() {
          const [model, vocabulary] = await Promise.all([
              this.loadModel(),
              loadVocabulary(`${BASE_PATH}/vocab.json?tfjs-format=file`)
          ]);
          this.model = model;
          this.tokenizer = new Tokenizer(vocabulary, RESERVED_SYMBOLS_COUNT$1);
      }
      /**
       *
       * Returns a map of queryEmbedding and responseEmbedding
       *
       * @param input the ModelInput that contains queries and answers.
       */
      embed(input) {
          const embeddings = tf.tidy(() => {
              const queryEncoding = this.tokenizeStrings(input.queries, INPUT_LIMIT);
              const responseEncoding = this.tokenizeStrings(input.responses, INPUT_LIMIT);
              if (input.contexts != null) {
                  if (input.contexts.length !== input.responses.length) {
                      throw new Error('The length of response strings ' +
                          'and context strings need to match.');
                  }
              }
              const contexts = input.contexts || [];
              if (input.contexts == null) {
                  contexts.length = input.responses.length;
                  contexts.fill('');
              }
              const contextEncoding = this.tokenizeStrings(contexts, INPUT_LIMIT);
              const modelInputs = {};
              modelInputs[QUERY_NODE_NAME] = queryEncoding;
              modelInputs[RESPONSE_NODE_NAME] = responseEncoding;
              modelInputs[RESPONSE_CONTEXT_NODE_NAME] = contextEncoding;
              return this.model.execute(modelInputs, [QUERY_RESULT_NODE_NAME, RESPONSE_RESULT_NODE_NAME]);
          });
          const queryEmbedding = embeddings[0];
          const responseEmbedding = embeddings[1];
          return { queryEmbedding, responseEmbedding };
      }
      tokenizeStrings(strs, limit) {
          const tokens = strs.map(s => this.shiftTokens(this.tokenizer.encode(s), INPUT_LIMIT));
          return tf.tensor2d(tokens, [strs.length, INPUT_LIMIT], 'int32');
      }
      shiftTokens(tokens, limit) {
          tokens.unshift(TOKEN_START_VALUE);
          for (let index = 0; index < limit; index++) {
              if (index >= tokens.length) {
                  tokens[index] = TOKEN_PADDING;
              }
              else if (!SKIP_VALUES.includes(tokens[index])) {
                  tokens[index] += OFFSET;
              }
          }
          return tokens.slice(0, limit);
      }
  }

  /**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   */
  const BASE_PATH$1 = 'https://storage.googleapis.com/tfjs-models/savedmodel/universal_sentence_encoder';
  async function load() {
      const use = new UniversalSentenceEncoder();
      await use.load();
      return use;
  }
  class UniversalSentenceEncoder {
      async loadModel() {
          return tfconv.loadGraphModel('https://tfhub.dev/tensorflow/tfjs-model/universal-sentence-encoder-lite/1/default/1', { fromTFHub: true });
      }
      async load() {
          const [model, vocabulary] = await Promise.all([this.loadModel(), loadVocabulary(`${BASE_PATH$1}/vocab.json`)]);
          this.model = model;
          this.tokenizer = new Tokenizer(vocabulary);
      }
      /**
       *
       * Returns a 2D Tensor of shape [input.length, 512] that contains the
       * Universal Sentence Encoder embeddings for each input.
       *
       * @param inputs A string or an array of strings to embed.
       */
      async embed(inputs) {
          if (typeof inputs === 'string') {
              inputs = [inputs];
          }
          const encodings = inputs.map(d => this.tokenizer.encode(d));
          const indicesArr = encodings.map((arr, i) => arr.map((d, index) => [i, index]));
          let flattenedIndicesArr = [];
          for (let i = 0; i < indicesArr.length; i++) {
              flattenedIndicesArr =
                  flattenedIndicesArr.concat(indicesArr[i]);
          }
          const indices = tf.tensor2d(flattenedIndicesArr, [flattenedIndicesArr.length, 2], 'int32');
          const values = tf.tensor1d(tf.util.flatten(encodings), 'int32');
          const modelInputs = { indices, values };
          const embeddings = await this.model.executeAsync(modelInputs);
          indices.dispose();
          values.dispose();
          return embeddings;
      }
  }

  exports.load = load;
  exports.UniversalSentenceEncoder = UniversalSentenceEncoder;
  exports.Tokenizer = Tokenizer;
  exports.loadQnA = loadQnA;
  exports.version = version;

  Object.defineProperty(exports, '__esModule', { value: true });

})));
